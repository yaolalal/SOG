# åŠ è½½æ•°æ®ï¼ˆjsonæ–‡ä»¶ï¼‰
import json
import numpy as np
import os
import re
import torch,multiprocessing
from datasets import DatasetDict,Dataset
from transformers import AutoTokenizer


dataset_name = 'cora'
data_path = '/home/wujingyao/codes/experiment/Quantization/VQGraph-my/cora.json'
with open(data_path, 'r') as f:
    data = json.load(f)
print("ğŸŒŸLoaded {} nodes. Data format:".format(len(data)))
print(data[0])


tokenizer_path = "/data1/wujingyao-20354/wujingyao/codes/models/llama-3.2-3b-instruct-graph/update_tokenizer"
tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)
assistant_name = "LLM Assistant"
system_prompt = "You are a helpful and reliable assistant."
format_system_prompt = "<|start_header_id|>system<|end_header_id|>\n\n"+system_prompt+"<|eot_id|>"
tokenizer.chat_template = "<|begin_of_text|>"+format_system_prompt+"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>"+assistant_name+"<|end_header_id|>\n\n' }}{% endif %}"


prompt = """Classify the node into one of above categories:
Text attribute: {text_attribute}
Ego Graph: <struct>{structure_token}</struct>
Neighbors attributes:
{one_hop_str}{two_hop_str}
"""

def extract_title_and_abstract(data_list):
    """
    ä»åŒ…å« 'raw_text' å­—æ®µçš„å­—å…¸åˆ—è¡¨ä¸­æå–æ ‡é¢˜å’Œæ‘˜è¦ã€‚

    'raw_text' å­—æ®µçš„æœŸæœ›æ ¼å¼ä¸º:
    'Title: <æ‚¨çš„æ ‡é¢˜å†…å®¹>  \tAbstract: <æ‚¨çš„æ‘˜è¦å†…å®¹>'

    å‚æ•°:
    data_list (list): ä¸€ä¸ªå­—å…¸åˆ—è¡¨ã€‚æ¯ä¸ªå­—å…¸é¢„è®¡åŒ…å«ä¸€ä¸ª 'raw_text' é”®ã€‚

    è¿”å›:
    list: ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«æˆåŠŸæå–çš„ 'title' å’Œ 'abstract'ã€‚
          å¦‚æœæŸä¸ªæ¡ç›®æ— æ³•æå–ï¼Œåˆ™ä¼šè·³è¿‡è¯¥æ¡ç›®ã€‚
    """

    # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼è§£é‡Š:
    # - r"Title:": åŒ¹é…å­—ç¬¦ä¸² "Title:"ã€‚
    # - (?P<title>.*?)":
    #   - (?P<title>...) : æ•è·åŒ¹é…çš„å†…å®¹åˆ°åä¸º 'title' çš„ç»„ä¸­ã€‚
    #   - .*? : åŒ¹é…ä»»æ„å­—ç¬¦ ('.') é›¶æ¬¡æˆ–å¤šæ¬¡ ('*')ï¼Œä½†å°½å¯èƒ½å°‘åœ°åŒ¹é… ('?') (éè´ªå©ªæ¨¡å¼)ã€‚
    #           è¿™ç¡®ä¿å®ƒåœ¨é‡åˆ° "\s*\tAbstract:" ä¹‹å‰åœæ­¢ã€‚
    # - \s*: åŒ¹é…é›¶ä¸ªæˆ–å¤šä¸ªç©ºç™½å­—ç¬¦ (ä¾‹å¦‚ç©ºæ ¼ã€åˆ¶è¡¨ç¬¦æœ¬èº«ä¹‹å¤–çš„ç©ºç™½)ã€‚
    # - \t: åŒ¹é…ä¸€ä¸ªåˆ¶è¡¨ç¬¦ã€‚
    # - Abstract:: åŒ¹é…å­—ç¬¦ä¸² "Abstract:"ã€‚
    # - (?P<abstract>.*)":
    #   - (?P<abstract>...) : æ•è·åŒ¹é…çš„å†…å®¹åˆ°åä¸º 'abstract' çš„ç»„ä¸­ã€‚
    #   - .* : åŒ¹é…ä»»æ„å­—ç¬¦ ('.') é›¶æ¬¡æˆ–å¤šæ¬¡ ('*') (è´ªå©ªæ¨¡å¼ï¼Œä¼šåŒ¹é…åˆ°å­—ç¬¦ä¸²æœ«å°¾)ã€‚
    # - re.DOTALL: ä½¿ '.' ç‰¹æ®Šå­—ç¬¦å¯ä»¥åŒ¹é…æ¢è¡Œç¬¦ï¼Œè¿™æ ·æ ‡é¢˜æˆ–æ‘˜è¦å¯ä»¥è·¨è¶Šå¤šè¡Œã€‚
    pattern = re.compile(r"Title:(?P<title>.*?)\s*\tAbstract:(?P<abstract>.*)", re.DOTALL)
    not_found = 0
    not_found_list = []
    not_found_centers = []
    for item in data_list:
        if not isinstance(item, dict):
            # æ‚¨å¯ä»¥é€‰æ‹©è®°å½•æˆ–å¤„ç†éå­—å…¸ç±»å‹çš„æ¡ç›®
            print(f"è­¦å‘Š: è·³è¿‡éå­—å…¸æ¡ç›®: {item}")
            continue

        raw_text = item.get('raw_text')

        if not raw_text or not isinstance(raw_text, str):
            # 'raw_text' å­—æ®µç¼ºå¤±æˆ–ä¸æ˜¯å­—ç¬¦ä¸²ï¼Œè·³è¿‡
            print(f"è­¦å‘Š: è·³è¿‡ 'raw_text' ç¼ºå¤±æˆ–æ— æ•ˆçš„æ¡ç›®: {item}")
            continue

        match = pattern.search(raw_text)
        if match:
            # .strip() ç”¨äºå»é™¤æ•è·å†…å®¹ä¸¤ç«¯çš„ç©ºç™½å­—ç¬¦ (åŒ…æ‹¬æ¢è¡Œç¬¦å’Œå¤šä½™çš„ç©ºæ ¼)
            title = match.group("title").strip()
            abstract = match.group("abstract").strip()
            item['title'] = title
            item['abstract'] = abstract
        else:
            # å¦‚æœéœ€è¦ï¼Œå¯ä»¥è®°å½•é‚£äº› 'raw_text' æ ¼å¼ä¸åŒ¹é…çš„æ¡ç›®
            # print(f"ä¿¡æ¯: æœªåœ¨ä»¥ä¸‹ 'raw_text' ä¸­åŒ¹é…åˆ°æ¨¡å¼: {raw_text[:100]}...")
            not_found += 1
            not_found_list.append((raw_text, item['node']))
            not_found_centers.append(item['node'])
            stripped_line = raw_text.lstrip()
            if stripped_line.startswith("Title:"):
                item['title'] = stripped_line.split("Title:", 1)[1].strip()
                item['abstract'] = None
            elif stripped_line.startswith("Abstract:"):
                item['abstract'] = stripped_line.split("Abstract:", 1)[1].strip()
                item['title'] = None
            else:
                item['title'] = None
                item['abstract'] = None           
    print(f"æœªåŒ¹é…åˆ°æ¨¡å¼çš„æ¡ç›®æ•°: {not_found}")

    only_title = []
    only_abstract = []
    bad = []
    for (raw_text, node) in not_found_list:
        stripped_line = raw_text.lstrip()
        if stripped_line.startswith("Title:"):
            only_title.append(node)
        elif stripped_line.startswith("Abstract:"):
            only_abstract.append(node)
        else:
            bad.append(node)
    print(f"Only titles: {only_title}")
    print(f"Only abstracts: {only_abstract}")
    print(f"Bad: {bad}")

    return not_found_list,not_found_centers


def process(row,verbose=False):
   # 1.æ–‡æœ¬å±æ€§
   if row['title'] is not None or row['abstract'] is not None:
      text_attribute = row['raw_text']
   else:
      text_attribute = ''

   # 2.ç»“æ„ä¿¡æ¯
   structure_token = f'<struct_{row["structure_code"]}>'

   # 3.é‚»å±…å±æ€§
   one_hop_str = ''
   for i in range(len(row['one_hop_positions'])):
      neigh = row["one_hop_neighbors"][i]
      pos = row['one_hop_positions'][i]
      label = data[neigh]['label_name']
      if data[neigh]['title'] is not None:
         neigh_text = data[neigh]['title']
      elif data[neigh]['abstract'] is not None:
         neigh_text = data[neigh]['abstract'][:66]+'...'
      else:
         neigh_text = ''
         if verbose:
            print(f'Bad node appear in one_hop_neighbors rank {pos}:',row['node'])
      one_hop_str += f'One hop rank {pos} neigh attribute:{neigh_text}, label:{label},\n'
      
   two_hop_str = ''
   for i in range(len(row['two_hop_positions'])):
      neigh = row["two_hop_neighbors"][i]
      pos = row['two_hop_positions'][i]
      label = data[neigh]['label_name']
      if data[neigh]['title'] is not None:
         neigh_text = data[neigh]['title']
      elif data[neigh]['abstract'] is not None:
         neigh_text = data[neigh]['abstract'][:66]+'...'
      else:
         neigh_text = ''
         if verbose:
            print(f'Bad node appear in two_hop_neighbors rank {pos}:',row['node'])
      two_hop_str += f'Two hop rank {pos} neigh attribute:{neigh_text}, label:{label},\n'
   
   # 4.æ„å»ºæç¤º
   whole_text = prompt.format(text_attribute=text_attribute,
                              structure_token=structure_token,
                              one_hop_str=one_hop_str,
                              two_hop_str=two_hop_str)
   row['instruction'] = whole_text
   row['response'] = row['label_name']
          
   messages = [
      {"role": "user", "content": row['instruction']},
      {"role": assistant_name, "content": row['response']},
   ]
   row["text"] = tokenizer.apply_chat_template(messages, tokenize=False)+"<|end_of_text|>"
   return row

def train_val_test_split(dataset_name):
    f_path = f'/home/wujingyao/codes/ experiment/Quantization/VQGraph-my/new_node_feats/{dataset_name}/splits.json'
    with open(f_path,'r') as f:
        splits = json.load(f)
    train_indices = splits['train']
    val_indices = splits['val']
    test_indices = splits['test']
    return train_indices,val_indices,test_indices


def load_data(verbose=True):
    _,_ = extract_title_and_abstract(data)  
    dataset = Dataset.from_list(data)
    ds = dataset.map(
        process,
        num_proc= multiprocessing.cpu_count(),
        load_from_cache_file=False,
    )

    train_indices,val_indices,test_indices = train_val_test_split(dataset_name)
    train_dataset = ds.select(train_indices)
    val_dataset = ds.select(val_indices)
    test_dataset = ds.select(test_indices)

    ds_dict = DatasetDict({
        "train": train_dataset,
        "validation": val_dataset,
        "test": test_dataset
    })
    if verbose:
        print('ğŸŒŸFinal:',ds_dict)
        print('ğŸŒŸFormat:')
        print(ds_dict['train'][0]['text'])

    return ds_dict


def get_token_probability(model, input_tokens, target_token):
    with torch.no_grad():
        outputs = model(input_tokens)
    # get the logits for our model output
    logits = outputs.logits[:, -1, :]
    # calculate the softmax probabilities
    probs = torch.softmax(logits, dim=-1)
    token_prob = probs[0, target_token]
    return token_prob

def print_probabilibites_on_test(model, tokenizer, print_tokens):
    question = "Continue writing. The most important information of a graph is ." 
    question = "Ego Graph: <struct>"
    messages = [{"role": "user", "content": question}]
    tokens = tokenizer.apply_chat_template(messages, tokenize=True, return_tensors="pt", add_generation_prompt=True)
    tokens = tokens.to(model.device)

    for token in print_tokens:
        token_id = tokenizer.convert_tokens_to_ids(token)
        prob = get_token_probability(model, tokens, token_id)
        print(f"Token: {token}, Probability: {prob:.6f}")

def print_dict(dictionary):
    for key, value in dictionary.items():
        if type(value) == str:
            print(key,':\n', value)
        else:
            print(key,' : ', value)
def wrap_struct_token(example):
    # åŒ¹é…å½¢å¦‚ <gstruct_180> çš„ç»“æ„å¹¶æ›¿æ¢
    example_new = re.sub(r'(<struct_\d+>)', r'<struct>\1</struct>', example)
    return example_new

def get_tokenizer(tokenizer_path=None):
    if tokenizer_path is None:
        tokenizer_path = "/data1/wujingyao-20354/wujingyao/codes/models/llama-3.2-3b-instruct-graph/update_tokenizer"
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)
    assistant_name = "LLM Assistant"
    system_prompt = "You are a helpful and reliable assistant."
    format_system_prompt = "<|start_header_id|>system<|end_header_id|>\n\n"+system_prompt+"<|eot_id|>"
    tokenizer.chat_template = "<|begin_of_text|>"+format_system_prompt+"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>"+assistant_name+"<|end_header_id|>\n\n' }}{% endif %}"
    return tokenizer,assistant_name


if __name__ == '__main__':
    load_data()

    

